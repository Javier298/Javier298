{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ffc840-690c-4c01-a45f-c14f74394796",
   "metadata": {},
   "source": [
    "# Fine-Tuning Qwen2-VL-2B-Instruct on dacl-10k dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041ff5e-0393-4f7e-828c-a62aca11d3d3",
   "metadata": {},
   "source": [
    "### I will be fine-tuning the above mentioned VLM on the dataset\n",
    "\n",
    "### I have used this tutorial as my source: https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea5110-b238-4025-a8ed-a4122fe7a47b",
   "metadata": {},
   "source": [
    "#### The check-point shards of qwen are already downloaded. TO AVOID DOWNLOADING STUFF AGAIN, I have written the below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df64dab-10c7-48bf-8b4d-dcc1d90ffc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:/mdfBIM+ - VLM 4 Bridge Damages - Jäkel_Bitte nicht löschen!_/hugging_face/hub' \n",
    "#If check-point shards are not downloaded, then comment it####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44281482-00b3-4b49-9d8d-00c8ba928282",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb9fa3-1357-49e6-bd5c-1e41d6cf0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig, Trainer, TrainingArguments, get_scheduler\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft.optimizers import create_loraplus_optimizer\n",
    "import evaluate\n",
    "#import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, average_precision_score\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e69f45-88f4-4af1-a682-1bbf8df2482a",
   "metadata": {},
   "source": [
    "## Resizing the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695835d-a8b3-464a-943e-12e224b51d0a",
   "metadata": {},
   "source": [
    "#### The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation.\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83dad89-2163-41b4-a960-91e00f5bc4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image: Image.Image):\n",
    "    min_size = 56  # Ensure minimum 56X56\n",
    "    max_size = 1008 #Ensure max 1008X1008 #1120\n",
    "    width, height = image.size\n",
    "    \n",
    "    if width < min_size or height < min_size:\n",
    "        image = image.resize((max(width,min_size), max(height,min_size)), Image.BILINEAR)\n",
    "    elif width > max_size or height > max_size:\n",
    "        image = image.resize((min(width,max_size), min(height,max_size)), Image.BILINEAR)   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417e5fc-b4a6-448a-b990-a7b5b3dec6e0",
   "metadata": {},
   "source": [
    "## Parameters for *training_args*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176f854-cd36-413e-919c-777a7bf7e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "EPOCHS = 1#2\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_CHECKPOINTING = True  # Tradeoff between memory efficiency and computation time.\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 #4\n",
    "USE_REENTRANT = False\n",
    "OPTIM = \"paged_adamw_32bit\" #\"paged_adamw_32bit\"\n",
    "LEARNING_RATE = 5e-5 #2e-4 #5e-4\n",
    "LOGGING_STEPS = 433 #180 #173 #433 \n",
    "EVAL_STEPS = 433 #180 #173 #433 \n",
    "SAVE_STEPS = 866 #360 #346 #866 \n",
    "EVAL_STRATEGY = \"steps\"\n",
    "SAVE_STRATEGY = \"steps\" #\"steps\"\n",
    "METRIC_FOR_BEST_MODEL=\"eval_loss\" #\"eval_loss\"\n",
    "LOAD_BEST_MODEL_AT_END=True\n",
    "MAX_GRAD_NORM = 1\n",
    "WARMUP_RATIO = 0.1 #delete\n",
    "WARMUP_STEPS = 0 #delete\n",
    "WEIGHT_DECAY = 0.01 #delete\n",
    "DATASET_KWARGS={\"skip_prepare_dataset\": True} # We have to put for VLMs\n",
    "REMOVE_UNUSED_COLUMNS = False # VLM thing\n",
    "MAX_SEQ_LEN= 1024 #128\n",
    "NUM_STEPS = (6935 // (GRADIENT_ACCUMULATION_STEPS * BATCH_SIZE)) * EPOCHS #6935 #2882 #2777 \n",
    "print(f\"NUM_STEPS: {NUM_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1f7cd-34aa-4e36-82a1-cde766e435e7",
   "metadata": {},
   "source": [
    "## Mappings from numbers to damage and object types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6f317-99a2-42af-913a-20e244eb946e",
   "metadata": {},
   "source": [
    "Numbers from 12 to 17 (both included) are object types. Rest are damage types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2446a7-d89c-4ad0-94b6-dec4ee35ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "                                    0: \"Crack\",\n",
    "                                    1: \"ACrack\",\n",
    "                                    2: \"Wetspot\",\n",
    "                                    3: \"Efflorescence\",\n",
    "                                    4: \"Rust\",\n",
    "                                    5: \"Rockpocket\",\n",
    "                                    6: \"Hollowareas\",\n",
    "                                    7: \"Cavity\",\n",
    "                                    8: \"Spalling\",\n",
    "                                    9: \"Graffiti\",\n",
    "                                    10: \"Weathering\",\n",
    "                                    11: \"Restformwork\",\n",
    "                                    12: \"ExposedRebars\",\n",
    "                                    13: \"Bearing\",\n",
    "                                    14: \"EJoint (Expansion Joint)\",\n",
    "                                    15: \"Drainage\",\n",
    "                                    16: \"PEquipment (Protective Equipment)\",\n",
    "                                    17: \"JTape (Joint Tape)\",\n",
    "                                    18: \"Concrete Corrosion (ConcreteC)\",\n",
    "                                    19: \"Corrosion, no rust staining\",\n",
    "                                    20: \"NO Exposed Reinforcement\",\n",
    "                                    21: \"Scaling\",\n",
    "                                    22: \"General Defects\",\n",
    "                                    23: \"No defect\"\n",
    "                                    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7ad7f-14f6-4776-8f2d-2fc6b5ee1650",
   "metadata": {},
   "source": [
    "## Text Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bca0c-0822-4a8a-9b64-8fab6ddb015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in analyzing, describing, and interpreting visual data. \n",
    "You have currently learned about several bridge-damage types. Your task is to generate a short inspection report on seeing the image.\"\"\"\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": resize_image(Image.open(sample[\"image\"])),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"Here is the label-mapping of numbers to damage types {label_mapping}. Numbers 12 to 17(both included) are object types. Using the numbers, state the damage type(s) and object type(s) present in the image:\"\"\"\n",
    "                 },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\":sample[\"new_label\"] }], #'The supervisor will check' #sample[\"label\"]\n",
    "        },\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef484b-a781-4b70-a201-9793893489c4",
   "metadata": {},
   "source": [
    "## Loading the DACL dataset\n",
    "\n",
    "#### You can find the download link here: https://github.com/phiyodr/dacl10k-toolkit\n",
    "\n",
    "#### I used those images, but I made my own annotation files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e71b6-050a-4523-bd9c-6481e61c94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "dataset3 = load_dataset(\"json\", data_files = {'train':'D:/mdfBIM+ - VLM 4 Bridge Damages - Jäkel_Bitte nicht löschen!_/labels_for_two_datasets/Train/dacl/dacl_labels_train.json',\n",
    "                                                  'val': 'D:/mdfBIM+ - VLM 4 Bridge Damages - Jäkel_Bitte nicht löschen!_/labels_for_two_datasets/Val/dacl/dacl_labels_val.json'})\n",
    "\n",
    "#use image_cast from hf library\n",
    "\n",
    "train_dataset3 = dataset3['train']\n",
    "#train_label3 = train_dataset3[\"label\"]\n",
    "\n",
    "val_dataset3 = dataset3['val']\n",
    "\n",
    "train_dataset3 = [format_data(sample) for sample in train_dataset3]\n",
    "val_dataset3 = [format_data(sample) for sample in val_dataset3]\n",
    "#'''\n",
    "print(\"Formatted!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5c865-ea36-44a5-b4f8-fa02a0c1052d",
   "metadata": {},
   "source": [
    "#### I have commented this stuff, cuz I am not using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fbaf7-caba-4b9c-a4f8-664c356100bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################COMBINE DATASETS############################################\n",
    "'''\n",
    "combined_train_dataset = interleave_datasets([train_dataset,train_dataset2], probabilities=[0.5,0.5])\n",
    "combined_val_dataset = interleave_datasets([val_dataset,val_dataset2], probabilities=[0.5,0.5])\n",
    "print(\"Combined successfully!\")\n",
    "combined_train_dataset = [format_data(sample) for sample in combined_train_dataset]\n",
    "combined_val_dataset = [format_data(sample) for sample in combined_val_dataset]\n",
    "print(\"Formatted Successfully\")\n",
    "'''\n",
    "################################# checking ############################################\n",
    "#sample_data = train_dataset[0]\n",
    "#sample_question = train_dataset[0][1][\"content\"][1][\"text\"]\n",
    "#sample_answer = train_dataset[0][2][\"content\"][0][\"text\"]\n",
    "#sample_image = train_dataset[0][1][\"content\"][0][\"image\"]\n",
    "\n",
    "#print(sample_question)\n",
    "#print(sample_answer)\n",
    "#sample_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b04ec-7c68-4397-a0a6-2bd50b0e9e0a",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de764d0d-cbca-490b-854b-6f3780d572cd",
   "metadata": {},
   "source": [
    "I have just defined **bits&bytes**, but  I am not using it anywhere. I am not able to use it with **flash-attn2** parallely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc42b0e-7127-4fa2-a3d4-46aac9ef5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        #bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "     torch_dtype=torch.bfloat16,\n",
    "     attn_implementation=\"flash_attention_2\",\n",
    "     #load_in_4bit=True,\n",
    "     low_cpu_mem_usage=True,\n",
    "     #quantization_config=bnb_config,\n",
    "     device_map=\"auto\",\n",
    "     use_cache=False,\n",
    " )\n",
    "\n",
    "min_pixels = 4 * 28 * 28\n",
    "max_pixels = 1296 * 28 * 28 #1849 #1296 #1600\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels) #min_pixels=min_pixels\n",
    "processor.tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe4018-6a0d-4b02-814b-fc677894309e",
   "metadata": {},
   "source": [
    "#### Not using the below function anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8022ca-05ff-4ec6-ba72-d7e4d429add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### text generator ########################\n",
    "def text_generator(sample_data):\n",
    "    text = processor.apply_chat_template(\n",
    "        sample_data[0:2], tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    #print(f\"Prompt: {text}\")\n",
    "    #print(\"-\"*30)\n",
    "\n",
    "    image_inputs, _ = process_vision_info(sample_data) #sample_data[1][\"content\"][0][\"image\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images = image_inputs,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(device=model.device)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=MAX_SEQ_LEN)\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )\n",
    "    del inputs\n",
    "    actual_answer = sample_data[2][\"content\"][0][\"text\"]\n",
    "    return output_text[0], actual_answer\n",
    "    \n",
    "\n",
    "#generated_text, actual_answer = text_generator(sample_data)\n",
    "#print(f\"Generated Answer: {generated_text}\")\n",
    "#print(f\"Actual Answer: {actual_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72111b75-890c-4154-86ca-eda98876bb51",
   "metadata": {},
   "source": [
    "## LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd1a45-c18d-430b-80dd-d151a8828f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################LORA CONFIG############################\n",
    "peft_config = LoraConfig(\n",
    "    use_dora=True,\n",
    "    inference_mode=False,\n",
    "    lora_alpha=64, #16 #64\n",
    "    lora_dropout=0.1, #0.05\n",
    "    r=16, #16 #8\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], #\"k_proj\", #\"o_proj\" #, \"qkv\", \"proj\"\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights= \"eva\", #\"eva\"\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "#print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model = torch.compile(peft_model)\n",
    "\n",
    "peft_model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ef6ca-37cc-429c-8c69-44b633dc15ce",
   "metadata": {},
   "source": [
    "## Using SFTConfig function for training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10b7cb-cdd1-47c7-9631-fd878cc95dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################TRAINING ARGS################################################\n",
    "training_args = SFTConfig(\n",
    "    ##change this accordingly####\n",
    "    output_dir=\"./fine_tuned_weights/dacl_with_metrics_final\", #260156 #codebrim #codebrim_and_2601506\n",
    "    #label_names= train_label,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type= \"cosine_with_restarts\",\n",
    "    lr_scheduler_kwargs= {\"num_cycles\": 2}, #{\"power\": 3} #\"constant\", #\"linear\", #\"polynomial\", #\"cosine_with_restarts\"\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    metric_for_best_model=METRIC_FOR_BEST_MODEL,\n",
    "    load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    #warmup_ratio=WARMUP_RATIO,\n",
    "    #warmup_steps=WARMUP_STEPS,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    gradient_accumulation_steps=4, #16 #8 #4 #2\n",
    "    dataset_kwargs=DATASET_KWARGS,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    remove_unused_columns = REMOVE_UNUSED_COLUMNS,\n",
    "    optim=OPTIM,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\", #\"wandb\"\n",
    ")\n",
    "\n",
    "wandb.init(project=\"VQA or Image Captioning\", name=\"21.03.2025-dataset dacl_with_metrics-Qwen-2-VL-Instruct-2B\", config=training_args,) #Qwen-2-VL-Instruct-2B-dataset 2601506-18.02.2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca834c49-967a-4b60-8b65-fe5949008314",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e7d02-d5b6-4b48-b2f9-c03699c2f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### COLLATE FUNCTION #############################################\n",
    "#collate_sample = [train_dataset[0], train_dataset[1]] # for batch size 2.\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "    #image_inputs = [example[1][\"content\"][0][\"image\"] for example in examples] #[resize_image(example[1][\"content\"][0][\"image\"]) for example in examples]\n",
    "\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]\n",
    "    #resize_image(image_inputs)\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    #batch[\"input_ids\"]\n",
    "    #batch[\"labels\"] = batch[\"input_ids\"]\n",
    "    #batch=batch.to(device=model.device)\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "#collated_data = collate_fn(collate_sample)\n",
    "#print(collated_data.keys())  # dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1daa50-23e4-425b-921b-08d48d76f482",
   "metadata": {},
   "source": [
    "## Metric Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008782b5-0af1-40df-90af-ebf1f203431c",
   "metadata": {},
   "source": [
    "#### I didn't use it here cuz it was giving me OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a194f-b706-46e9-8176-b2c8270e73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################METRIC_FUNCTION##############################################\n",
    "\n",
    "# Load evaluation metrics\n",
    "#bleu = evaluate.load(\"bleu\")\n",
    "#rouge = evaluate.load(\"rouge\")\n",
    "#meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "#accuracy_metric = evaluate.load(\"accuracy\")\n",
    "#precision_metric = evaluate.load(\"precision\")\n",
    "#recall_metric = evaluate.load(\"recall\")\n",
    "#f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(dataset):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    i=1\n",
    "\n",
    "    for sample_data in dataset:\n",
    "        generated_answer, actual_answer = text_generator(sample_data)\n",
    "\n",
    "        # Convert text labels to numeric values\n",
    "        predicted_label = int(generated_answer) # Default to -1 if unknown\n",
    "        actual_label = actual_answer \n",
    "\n",
    "        if predicted_label != -1 and actual_label != -1:  # Ensure valid labels\n",
    "            preds.append(predicted_label)\n",
    "            labels.append(actual_label)\n",
    "        print(i)\n",
    "        i+=1\n",
    "\n",
    "    print(f\"length of preds: {len(preds)}\")\n",
    "    return compute_metrics(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0432c43e-a457-4794-a39a-2a66c69f94a0",
   "metadata": {},
   "source": [
    "## SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91dc185-c295-430d-a479-d8d224d1b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################SFT TRAINER##################################################\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= train_dataset3, #train_dataset2 #combined_train_dataset\n",
    "    eval_dataset= val_dataset3, #val_dataset2 #combined_val_dataset\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    #optimizers = (optimizer,scheduler),\n",
    "    #compute_metrics=compute_metrics,  # Updated\n",
    "    tokenizer = processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb0e87-45f1-4960-8dcf-e4f675beef45",
   "metadata": {},
   "source": [
    "## Training/fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffd99a-e402-4d6d-bcc6-470337676293",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "######################################EVAL######################################################\n",
    "print(\"-\"*30)\n",
    "print(\"Initial Evaluation\")\n",
    "metric = trainer.evaluate()\n",
    "print(metric)\n",
    "print(\"-\"*30)\n",
    "\n",
    "print(\"Training\")\n",
    "trainer.train()\n",
    "print(\"-\"*30)\n",
    "trainer.save_model(training_args.output_dir)\n",
    "\n",
    "#dataset = \"dacl\"\n",
    "#metrics = evaluate_model(val_dataset)\n",
    "#print(f\"The evaluation metrics for the {dataset} dataset are: \\n {metrics}\") #print(metrics)\n",
    "\n",
    "print(\"Done and finished\")\n",
    "print(\"-\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
